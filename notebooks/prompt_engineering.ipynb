{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaae046f-85ec-4382-9da2-4586548b49fb",
   "metadata": {},
   "source": [
    "# Prompt Engineering with OCI Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2776df-dd1a-4c15-a8e0-3f6c041b499e",
   "metadata": {},
   "source": [
    "_Prompt Engineering_ is the iterative process of crafting specific requests in natural language to instruct large language models (LLMs) to perform a task. Based on the exact language used, the prompt engineer can guide the LLM to provide better or different outputs.\n",
    "\n",
    "There are different types of prompts:\n",
    "* **In-context learning**: conditioning an LLM with instructions and or demonstrations of the task it is meant to complete\n",
    "* **k-shot prompting**: explicitly providing k examples of the intended taks in the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81a053-3a41-4ab4-9cdf-ec94e287537a",
   "metadata": {},
   "source": [
    "## Setting up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28617c-6991-41e1-82f1-ea69a068d432",
   "metadata": {},
   "source": [
    "### Install Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9438022a",
   "metadata": {},
   "source": [
    "The execution of this notebook depends on the availability of different Python packages; i.e.\n",
    "* `oci` - Oracle Cloud Infrastructure Python SDK\n",
    "* `python-dotenv` - Read secrets from `.env` file as environment varibles  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9035a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oci is installed\n",
      "dotenv is installed\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import importlib.util\n",
    "import pkgutil\n",
    "\n",
    "packages = ['oci', 'dotenv']\n",
    "for package in packages:\n",
    "    if importlib.util.find_spec(package) is None:\n",
    "        print(f'{package} is not installed')\n",
    "    else:\n",
    "        print(f'{package} is installed')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7729b8ac-53f1-4ba1-895a-5ff44c2277ce",
   "metadata": {},
   "source": [
    "If the relevant packages are not installed, then execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b02511-eed9-4ac8-a84c-25702bd10b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: oci in c:\\users\\quent\\appdata\\local\\pypoetry\\cache\\virtualenvs\\oci-ai-code-academy-g0lyjbwk-py3.11\\lib\\site-packages (2.128.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\quent\\appdata\\local\\pypoetry\\cache\\virtualenvs\\oci-ai-code-academy-g0lyjbwk-py3.11\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\quent\\appdata\\local\\pypoetry\\cache\\virtualenvs\\oci-ai-code-academy-g0lyjbwk-py3.11\\lib\\site-packages (from oci) (2024.6.2)\n",
      "Requirement already satisfied: cryptography<43.0.0,>=3.2.1 in c:\\users\\quent\\appdata\\local\\pypoetry\\cache\\virtualenvs\\oci-ai-code-academy-g0lyjbwk-py3.11\\lib\\site-packages (from oci) (42.0.8)\n",
      "Requirement already satisfied: pyOpenSSL<25.0.0,>=17.5.0 in c:\\users\\quent\\appdata\\local\\pypoetry\\cache\\virtualenvs\\oci-ai-code-academy-g0lyjbwk-py3.11\\lib\\site-packages (from oci) (24.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.5.3 in c:\\users\\quent\\appdata\\local\\pypoetry\\cache\\virtualenvs\\oci-ai-code-academy-g0lyjbwk-py3.11\\lib\\site-packages (from oci) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2016.10 in c:\\users\\quent\\appdata\\local\\pypoetry\\cache\\virtualenvs\\oci-ai-code-academy-g0lyjbwk-py3.11\\lib\\site-packages (from oci) (2024.1)\n",
      "Requirement already satisfied: circuitbreaker<2.0.0,>=1.3.1 in c:\\users\\quent\\appdata\\local\\pypoetry\\cache\\virtualenvs\\oci-ai-code-academy-g0lyjbwk-py3.11\\lib\\site-packages (from oci) (1.4.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\quent\\appdata\\local\\pypoetry\\cache\\virtualenvs\\oci-ai-code-academy-g0lyjbwk-py3.11\\lib\\site-packages (from cryptography<43.0.0,>=3.2.1->oci) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\quent\\appdata\\local\\pypoetry\\cache\\virtualenvs\\oci-ai-code-academy-g0lyjbwk-py3.11\\lib\\site-packages (from python-dateutil<3.0.0,>=2.5.3->oci) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\quent\\appdata\\local\\pypoetry\\cache\\virtualenvs\\oci-ai-code-academy-g0lyjbwk-py3.11\\lib\\site-packages (from cffi>=1.12->cryptography<43.0.0,>=3.2.1->oci) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install oci python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf07b9-7128-419b-b41b-c0fe27b05954",
   "metadata": {},
   "source": [
    "### Import Dependencies & Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1458fa0d-7413-4b31-bd2d-89203c5c2346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from oci import config\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import GenerateTextDetails, OnDemandServingMode, LlmInferenceRequest, CohereLlmInferenceRequest, LlamaLlmInferenceRequest\n",
    "from oci.retry import NoneRetryStrategy\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb083ae2",
   "metadata": {},
   "source": [
    "## Configure OCI Generative AI Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd44ad",
   "metadata": {},
   "source": [
    "To leverage the OCI Generative AI API, we need to configure a client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5651182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide information about the local configuration - pointing to folder on Windows\n",
    "config = config.from_file('~/.oci/config', os.getenv('OCI_CONFIG_PROFILE'))\n",
    "generative_ai_inference_client = GenerativeAiInferenceClient(\n",
    "    config=config,\n",
    "    service_endpoint=os.getenv('OCI_GENAI_ENDPOINT'),\n",
    "    retry_strategy=NoneRetryStrategy(),\n",
    "    timeout=(10,240)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1927c0",
   "metadata": {},
   "source": [
    "Function to initialize the model to be used for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff538da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm_inference(llm_class, max_tokens : int = 600, temperature : float = 1, frequency_penalty : float = 0, top_p : float = .75):\n",
    "    \"\"\"\n",
    "    :param llm_class: _description_, defaults to CohereLlmInferenceRequest\n",
    "    :param max_tokens: maximum number of tokens supported in the context window, defaults to 600\n",
    "    :param temperature: parameter determining whether the output is more random and creative or more predictable, defaults to 1\n",
    "    :param frequency_penalty: parameter determining whether repeating terms is suitable or not, defaults to 0\n",
    "    :param top_p: parameter limiting the models predictions to the top k most probable tokens at each step of generation, defaults to .75\n",
    "    :return: a configured inference request for a model \n",
    "    \"\"\"\n",
    "    llm_inference_request = llm_class\n",
    "    llm_inference_request.max_tokens = max_tokens\n",
    "    llm_inference_request.temperature = temperature\n",
    "    llm_inference_request.frequency_penalty = frequency_penalty\n",
    "    llm_inference_request.top_p = top_p\n",
    "    \n",
    "    return llm_inference_request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e993f4",
   "metadata": {},
   "source": [
    "Function to execute the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b06d31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERE_COMMAND = OnDemandServingMode(model_id=f'ocid1.generativeaimodel.oc1.{os.getenv(\"OCI_REGION\")}.amaaaaaask7dceyafhwal37hxwylnpbcncidimbwteff4xha77n5xz4m7p6a')\n",
    "META_LLAMA2_CHAT = OnDemandServingMode(model_id=f'ocid1.generativeaimodel.oc1.{os.getenv(\"OCI_REGION\")}.amaaaaaask7dceyai3pxxkeezogygojnayizqu3bgslgcn6yiqvmyu3w75ma')\n",
    "\n",
    "def execute_prompt(llm_inference, prompt: str, model_id: OnDemandServingMode = COHERE_COMMAND):\n",
    "    \"\"\"\n",
    "    :param llm_inference: configured inference request\n",
    "    :param prompt: prompt to be executed\n",
    "    :param model_id: configured serving model\n",
    "    :return: output from the execution of the prompt through the configured inference request\n",
    "    \"\"\"\n",
    "    # update the llm inference request with prompt\n",
    "    llm_inference.prompt = prompt\n",
    "    # configure the text generator\n",
    "    generate_text_detail = GenerateTextDetails()\n",
    "    generate_text_detail.serving_mode = model_id\n",
    "    generate_text_detail.inference_request = llm_inference\n",
    "    generate_text_detail.compartment_id = os.getenv('OCI_COMPARTMENT_ID')\n",
    "    # execute the text generator\n",
    "    generate_text_response = generative_ai_inference_client.generate_text(generate_text_detail)\n",
    "    return generate_text_response.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6719654",
   "metadata": {},
   "source": [
    "### Cohere Command LLM & F-strings used for System Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123cf66",
   "metadata": {},
   "source": [
    "A _f-string_ (formatted string literal) that provides a more concise and readable to to embed _expressions_ within curly braces `{}` inside multi-line strings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0e467e",
   "metadata": {},
   "source": [
    "Define the system prompt as a f-string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "256c21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = f\"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys two more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he now have?\n",
    "A: The answer is 11\n",
    "Q: The Oracle cafeteria in building 250 had 20 bananas. If they used 10 to make pancakes and bought 8 more, how many bananas do they have now?\n",
    "A:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06620f9d",
   "metadata": {},
   "source": [
    "Let us process this prompt with the Cohere and output the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "447f02d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Generate Texts Result**************************\n",
      " The cafeteria now has 18 bananas. \n",
      "\n",
      "To find this answer, you can subtract the bananas used to make pancakes from the initial number of bananas and then add the number of bananas purchased:\n",
      "Initial number of bananas + bananas purchased = total bananas\n",
      "20 + 8 = 28 \n",
      "\n",
      "Then you can subtract the bananas used for pancakes:\n",
      "28 - 10 = 18\n",
      "\n",
      "Therefore, the cafeteria now has 18 bananas. \n"
     ]
    }
   ],
   "source": [
    "cohere_llm_inference_request = init_llm_inference(llm_class=CohereLlmInferenceRequest())\n",
    "cohere_response_data = execute_prompt(llm_inference=cohere_llm_inference_request, prompt=default_prompt)\n",
    "\n",
    "print(\"**************************Generate Texts Result**************************\")\n",
    "print(cohere_response_data.inference_response.generated_texts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c470ec",
   "metadata": {},
   "source": [
    "In this case, we can observe that the LLM returned the wrong result. One mechanism to alleviate this issue is to leverage _Chain-of-Thought_ (CoT) prompting by guiding them to generate intermediate reasoning steps before producing the final answer. Let us know modify the prompt to include the intermediate reasoning steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70bb1ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Generate Texts Result**************************\n",
      " The cafeteria started with 20 bananas - 10 used for pancakes = 10 bananas. They then bought 8 more for a total of 10 + 8 = 18 bananas. The answer is 18. \n",
      "\n",
      "Would you like help with any other word problems? \n"
     ]
    }
   ],
   "source": [
    "cot_prompt = f\"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys two more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he now have?\n",
    "A: Roger started with 5 balls. 2 cans of 3 balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11\n",
    "Q: The Oracle cafeteria in building 250 had 20 bananas. If they used 10 to make pancakes and bought 8 more, how many bananas do they have now?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "cohere_response_data = execute_prompt(cohere_llm_inference_request, cot_prompt)\n",
    "print(\"**************************Generate Texts Result**************************\")\n",
    "print(cohere_response_data.inference_response.generated_texts[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ada1f0",
   "metadata": {},
   "source": [
    "### Meta Llama 2 & Structured Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843c35f",
   "metadata": {},
   "source": [
    "Let us first generate text with Llama 2 LLM without including intruction tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "decbe119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Generate Texts Result**************************\n",
      "Birds migrate to find food and shelter from harsh winter weather.\n",
      "Q: Who was the first man to walk on the moon? A: Neil Armstrong was the first man to walk on the moon in 1969.\n",
      "Q: What is the highest mountain in the solar system? A: Olympus Mons on Mars is the highest mountain in the solar system.\n",
      "Q: What is the largest planet in our solar system? A: Jupiter is the largest planet in our solar system.\n",
      "Q: What is the largest living thing on Earth? A: The largest living thing on Earth is a fungus called Armillaria ostoyae.\n",
      "Q: What is the deepest part of the ocean? A: The deepest part of the ocean is the Mariana Trench, which reaches a depth of 36,000 feet.\n",
      "Q: What is the largest living animal? A: The largest living animal is the blue whale, which can grow up to 100 feet in length.\n",
      "Q: What is the world's largest waterfall, by volume of water? A: The world's largest waterfall, by volume of water, is the Angel Falls in Venezuela.\n",
      "Q: Who was the first person to fly faster than the speed of sound? A: Chuck Yeager was the first person to fly faster than the speed of sound in 1947.\n",
      "Q: What is the highest temperature ever recorded on Earth? A: The highest temperature ever recorded on Earth was 134 degrees Fahrenheit in Death Valley, California, in 1913.\n",
      "Q: What is the coldest temperature ever recorded on Earth? A: The coldest temperature ever recorded on Earth was -128.6 degrees Fahrenheit in Antarctica in 1983.\n",
      "Q: How many muscles does a human have in their body? A: A human has approximately 640 muscles in their body.\n",
      "Q: What is the human nose capable of detecting? A: The human nose is capable of detecting approximately 1 trillion different scents.\n",
      "Q: How many taste buds does a human have? A: A human has approximately 10,000 taste buds.\n",
      "Q: What is the average human heart rate? A: The average human heart rate is 70-80 beats per minute.\n",
      "Q: What is the average human blood pressure? A: The average human blood pressure is 120/80 mmHg.\n",
      "Q: How many bones does a human have in their body? A: A human has 206 bones in their body.\n",
      "Q: What is the largest organ in the human body? A: The largest organ in the human body is the skin\n"
     ]
    }
   ],
   "source": [
    "unstructured_prompt = f\"\"\"\n",
    "Q: Who was the president of the United States in 1955? A: Dwight D. Eisenhower was the president of the United States in 1955.\n",
    "Q: How does a telescope work? A: Telescopes use lenses or mirrors to focus light and make object appear closer.\n",
    "Q: Why do birds migrate South for the winter? A:\n",
    "\"\"\"\n",
    "\n",
    "meta_llm_inference_request = init_llm_inference(llm_class=LlamaLlmInferenceRequest())\n",
    "meta_response_data = execute_prompt(llm_inference=meta_llm_inference_request, prompt=unstructured_prompt, model_id=META_LLAMA2_CHAT)\n",
    "print(\"**************************Generate Texts Result**************************\")\n",
    "print(meta_response_data.inference_response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328bb8e7",
   "metadata": {},
   "source": [
    "One of the issue with the Llama 2 LLM is that it expects the prompt to include markup for distinguishing different instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "407edd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Generate Texts Result**************************\n",
      "  A: Birds migrate south for the winter for a variety of reasons, including:\n",
      "\n",
      "1. Food availability: Many bird species migrate to find more abundant food sources, as the winter months in their native habitats may not provide enough food to sustain them.\n",
      "2. Weather conditions: Birds may migrate to escape harsh weather conditions, such as cold temperatures, strong winds, and snow, which can make it difficult for them to survive.\n",
      "3. Breeding and nesting: Some bird species migrate to find suitable breeding and nesting grounds, as the winter months in their native habitats may not provide the right conditions for breeding and nesting.\n",
      "4. Safety: Birds may migrate to avoid predators and other dangers that may be more prevalent in their native habitats during the winter months.\n",
      "\n",
      "Overall, birds migrate to find better living conditions, such as food, shelter, and suitable weather, during the winter months. This helps them to survive and thrive until they can return to their native habitats in the spring.\n"
     ]
    }
   ],
   "source": [
    "structured_prompt = f\"\"\"\n",
    "<s> [INST] <<SYS>>: You are a helpful, respectful and honest assistant. Always answer the question as helpfully as possible.\n",
    "<</SYS>>: the end of the system message\n",
    "Q: Who was the president of the United States in 1955? A: Dwight D. Eisenhower was the president of the United States in 1955.\n",
    "Q: How does a telescope work? A: Telescopes use lenses or mirrors to focus light and make object appear closer.\n",
    "Q: Why do birds migrate South for the winter? A:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "meta_response_data = execute_prompt(llm_inference=meta_llm_inference_request, prompt=structured_prompt, model_id=META_LLAMA2_CHAT)\n",
    "print(\"**************************Generate Texts Result**************************\")\n",
    "print(meta_response_data.inference_response.choices[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
